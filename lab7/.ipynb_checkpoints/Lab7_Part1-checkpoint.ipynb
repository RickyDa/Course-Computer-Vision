{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vkm0n-8HszP2"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "from torch.optim import SGD,Adam\n",
    "import numpy as np  \n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ehz4FI2f8sXM"
   },
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.MNIST(root='',train=True, download=True)\n",
    "num_classes = len(trainset.classes)\n",
    "# plt.imshow(trainset.data[np.random.randint(0,len(trainset))].numpy(),cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bqTwqRxl8fHf"
   },
   "outputs": [],
   "source": [
    "class LeNetModel(nn.Module):\n",
    "    param_conv1 = {'in_channels': 1,\n",
    "            'out_channels': 8,\n",
    "            'kernel_size':(5,5)}\n",
    "    max_pooling_kize1 = 2\n",
    "\n",
    "    param_conv2 = {'in_channels': 8,\n",
    "            'out_channels': 16,\n",
    "            'kernel_size':(5,5)}\n",
    "    max_pooling_kize2 = 2\n",
    "    param_fc1 = { 'in_features' :16*4*4, 'out_features':120}\n",
    "    param_fc2 = { 'in_features' : 120, 'out_features': 84}\n",
    "    param_fc3 = { 'in_features' : 84, 'out_features': num_classes}\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "         super(LeNetModel, self).__init__()\n",
    "         self.conv1 = nn.Conv2d(**self.param_conv1)\n",
    "         self.conv2 = nn.Conv2d(**self.param_conv2)\n",
    "\n",
    "         self.fc1 = nn.Linear(**self.param_fc1)\n",
    "         self.fc2 = nn.Linear(**self.param_fc2)\n",
    "         self.fc3 = nn.Linear(**self.param_fc3)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,self.max_pooling_kize1)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,self.max_pooling_kize2)\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        y = F.softmax(x,dim=1)\n",
    "        return y\n",
    "\n",
    "model_lenet = LeNetModel()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E93gpmGUxilL"
   },
   "source": [
    "# Optimizers\n",
    "### Refrences:\n",
    "SGD - https://pytorch.org/docs/master/_modules/torch/optim/sgd.html#SGD\n",
    "\n",
    "Adam - https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py\n",
    "\n",
    "RAdam - https://github.com/LiyuanLucasLiu/RAdam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hW1shd22stkL"
   },
   "outputs": [],
   "source": [
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:                    \n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size, exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss\n",
    "\n",
    "sgd = SGD(model_lenet.parameters(), lr=1e-3)\n",
    "adam = Adam(model_lenet.parameters())\n",
    "radam = RAdam(model_lenet.parameters())\n",
    "\n",
    "# OR Download the (radam.py) file from github to local directory\n",
    "\n",
    "# !wget https://raw.githubusercontent.com/LiyuanLucasLiu/RAdam/master/radam/radam.py\n",
    "# from radam import RAdam\n",
    "# radam = RAdam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lDkYiuXK8HCe"
   },
   "source": [
    "Focal Loss\n",
    "\n",
    "![alt text](https://miro.medium.com/max/848/1*Qrw4Xcem0hnlVg2Csk2FnQ.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jyk-O0bv8CaQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, reduction='mean', alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        CE_loss = F.binary_cross_entropy(inputs,\n",
    "                                         targets,\n",
    "                                         reduction=self.reduction)\n",
    "        prob_t = self.get_prob(CE_loss)\n",
    "        return self.alpha * (1-prob_t)**(self.gamma) * CE_loss\n",
    "\n",
    "    def get_prob(self,CE_loss):\n",
    "        # extract pt from -log(pt) == CE\n",
    "        return torch.exp(-CE_loss)\n",
    "\n",
    "class FocalLossWithLogits(FocalLoss):\n",
    "\n",
    "    def __init__(self,pos_weight=None, reduction='mean', alpha=1, gamma=2):\n",
    "        super(FocalLossWithLogits, self).__init__(reduction, alpha, gamma)\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        CE_loss = F.binary_cross_entropy_with_logits(inputs,\n",
    "                                                     targets,\n",
    "                                                     reduction=self.reduction,\n",
    "                                                     pos_weight=self.pos_weight)\n",
    "        prob_t = super().get_prob(CE_loss)\n",
    "        return self.alpha * (1-prob_t)**(self.gamma) * CE_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CWBRhBSAPZuW"
   },
   "source": [
    "# Test FocalLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ndP9oUrADF8"
   },
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "loss = FocalLoss()\n",
    "input_ = torch.randn(3, requires_grad=True)\n",
    "target = torch.empty(3).random_(2)\n",
    "output = loss(m(input_), target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TzGVjaO3Sy0S"
   },
   "source": [
    "# Test FocalLossWithLogits and RAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "colab_type": "code",
    "id": "TPlJ6lUa49sv",
    "outputId": "411d4ddc-fc17-4531-9679-45f74ac51a9b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 0.0765124037861824\n",
      "199 0.01782449707388878\n",
      "299 0.0032278629951179028\n",
      "399 0.0006381400744430721\n",
      "499 0.00015757900837343186\n"
     ]
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = FocalLossWithLogits(reduction='mean')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = RAdam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "\n",
    "    y_pred = model(x)\n",
    "\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ffT41cHYUiQO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NM5Q8ugxUo3x"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab7-Part1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
